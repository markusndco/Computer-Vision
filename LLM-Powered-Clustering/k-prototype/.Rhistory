}
# Apply the function to create a list of sentences
sentences <- purrr::pmap_chr(train_data, compile_text)
# Create an iterator over tokens
tokens <- itoken(sentences,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = TRUE)
# Create vocabulary
vocab <- create_vocabulary(tokens)
# Create a vectorizer
vectorizer <- vocab_vectorizer(vocab)
# Create a term-co-occurrence matrix (TCM)
tcm <- create_tcm(tokens, vectorizer)
# Fit a GloVe model
glove_model <- GlobalVectors$new(rank = 50, x_max = 10)
word_vectors <- glove_model$fit_transform(tcm, n_iter = 20)
# Combine main and context vectors
word_vectors <- word_vectors + t(glove_model$components)
# Create an embedding for each sentence by averaging word vectors
sentence_embeddings <- function(sent, word_vectors) {
words <- unlist(word_tokenizer(tolower(sent)))
valid_words <- words[words %in% rownames(word_vectors)]
if (length(valid_words) == 0) return(rep(NA, ncol(word_vectors)))
word_vecs <- word_vectors[valid_words, , drop = FALSE]
colMeans(word_vecs, na.rm = TRUE)
}
# Apply the embedding function to all sentences
embeddings <- t(sapply(sentences, sentence_embeddings, word_vectors = word_vectors))
# Remove sentences that couldn't be embedded
embeddings <- embeddings[complete.cases(embeddings), ]
# Convert embeddings to data frame
embeddings_df <- as.data.frame(embeddings)
# Inspect the resulting embeddings
str(embeddings_df)
# Save embeddings to CSV for future use
write.csv(embeddings_df, "train_embeddings.csv", row.names = FALSE)
# Load data
embedding_data <- read_excel("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/LMM+kMean/train_embeddings.csv")
# Load data
embedding_data <- read.csv("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/LMM+kMean/train_embeddings.csv")
rm(list=ls())
library(readxl)
library(dplyr)
library(tidyr)
library(text2vec)
library(purrr)
# Load data
train_data <- read_excel("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/train_data_no_outliers.xlsx", sheet = "train")
# Load data
train_data <- read_excel("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/train_data_no_outliers.xlsx")
# Print column names
cat("Column names in train_data:\n")
print(names(train_data))
library(mvoutlier)
# Identify outliers using z-score method
outliers <- mvoutlier::zscore(embeddings_df)
# Threshold for outliers (e.g., consider data points with z-score greater than 3 as outliers)
threshold <- 3
# Find indices of outliers
outlier_indices <- which(abs(outliers) > threshold, arr.ind = TRUE)
# Remove outliers from embedding data
embedding_data_no_outliers <- embeddings_df[-outlier_indices[,1], ]
# Inspect the resulting embeddings without outliers
str(embedding_data_no_outliers)
# Optionally, save the embedding data without outliers to CSV
write.csv(embedding_data_no_outliers, "train_embeddings_no_outliers.csv", row.names = FALSE)
# Load necessary libraries
library(readxl)
library(dplyr)
library(tidyr)
library(text2vec)
library(purrr)
# Load data
train_data <- read_excel("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/train.xlsx", sheet = "train")
# Print column names
cat("Column names in train_data:\n")
print(names(train_data))
# expected arguments in compile_text function
expected_arguments <- c("age", "housing", "job", "marital", "education", "default", "balance", "loan", "contact")
if (!all(expected_arguments %in% names(train_data))) {
stop("Column names in train_data do not match expected arguments in compile_text function.")
}
str(train_data)
# Removing NA values or any other preprocessing
train_data <- train_data %>% tidyr::drop_na()
# Convert to a list of sentences
compile_text <- function(age, housing, job, marital, education, default, balance, loan, contact) {
text <- paste0(
"Age: ", age, ", ",
"Housing load: ", housing, ", ",
"Job: ", job, ", ",
"Marital: ", marital, ", ",
"Education: ", education, ", ",
"Default: ", default, ", ",
"Balance: ", balance, ", ",
"Personal loan: ", loan, ", ",
"Contact: ", contact
)
return(text)
}
# Apply the function to create a list of sentences
sentences <- purrr::pmap_chr(train_data, compile_text)
# Create an iterator over tokens
tokens <- itoken(sentences,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = TRUE)
# Create vocabulary
vocab <- create_vocabulary(tokens)
# Create a vectorizer
vectorizer <- vocab_vectorizer(vocab)
# Create a term-co-occurrence matrix (TCM)
tcm <- create_tcm(tokens, vectorizer)
# Fit a GloVe model
glove_model <- GlobalVectors$new(rank = 50, x_max = 10)
word_vectors <- glove_model$fit_transform(tcm, n_iter = 20)
# Combine main and context vectors
word_vectors <- word_vectors + t(glove_model$components)
# Create an embedding for each sentence by averaging word vectors
sentence_embeddings <- function(sent, word_vectors) {
words <- unlist(word_tokenizer(tolower(sent)))
valid_words <- words[words %in% rownames(word_vectors)]
if (length(valid_words) == 0) return(rep(NA, ncol(word_vectors)))
word_vecs <- word_vectors[valid_words, , drop = FALSE]
colMeans(word_vecs, na.rm = TRUE)
}
# Apply the embedding function to all sentences
embeddings <- t(sapply(sentences, sentence_embeddings, word_vectors = word_vectors))
# Remove sentences that couldn't be embedded
embeddings <- embeddings[complete.cases(embeddings), ]
# Convert embeddings to data frame
embeddings_df <- as.data.frame(embeddings)
# Inspect the resulting embeddings
str(embeddings_df)
# Save embeddings to CSV for future use
write.csv(embeddings_df, "train_embeddings.csv", row.names = FALSE)
# Load data
embedding_data <- read.csv("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/LMM+kMean/train_embeddings.csv")
library(mvoutlier)
# Identify outliers using z-score method
outliers <- mvoutlier::zscore(embeddings_df)
# Threshold for outliers (e.g., consider data points with z-score greater than 3 as outliers)
threshold <- 3
# Find indices of outliers
outlier_indices <- which(abs(outliers) > threshold, arr.ind = TRUE)
# Remove outliers from embedding data
embedding_data_no_outliers <- embeddings_df[-outlier_indices[,1], ]
# Inspect the resulting embeddings without outliers
str(embedding_data_no_outliers)
# Optionally, save the embedding data without outliers to CSV
write.csv(embedding_data_no_outliers, "train_embeddings_no_outliers.csv", row.names = FALSE)
View(embedding_data)
View(embedding_data)
library(mvoutlier)
# Identify outliers using z-score method
outliers <- mvoutlier::zscore(embeddings_df)
# Threshold for outliers (e.g., consider data points with z-score greater than 3 as outliers)
threshold <- 3
# Find indices of outliers
outlier_indices <- which(abs(outliers) > threshold, arr.ind = TRUE)
# Calculate z-scores for each column
z_scores <- apply(embeddings_df, 2, function(x) (x - mean(x)) / sd(x))
# Calculate z-scores for each column
z_scores <- apply(embeddings_data, 2, function(x) (x - mean(x)) / sd(x))
# Calculate z-scores for each column
z_scores <- apply(embedding_data, 2, function(x) (x - mean(x)) / sd(x))
# Find indices of outliers
outlier_indices <- apply(z_scores, 2, function(x) which(abs(x) > threshold))
# Flatten the list of outlier indices
all_outlier_indices <- unlist(outlier_indices)
# Remove duplicate indices
all_outlier_indices <- unique(all_outlier_indices)
# Remove outliers from embedding data
embedding_data_no_outliers <- embedding_data[-all_outlier_indices, ]
# Inspect the resulting embeddings without outliers
str(embedding_data_no_outliers)
# Optionally, save the embedding data without outliers to CSV
write.csv(embedding_data_no_outliers, "train_embeddings_no_outliers.csv", row.names = FALSE)
# Load necessary libraries
library(readxl)
library(dplyr)
library(tidyr)
library(text2vec)
library(purrr)
# Load data
train_data <- read_csv("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/LMM+kMean/train_embeddings_no_outliers.csv)
# Print column names
cat("Column names in train_data:\n")
print(names(train_data))
# expected arguments in compile_text function
expected_arguments <- c("age", "housing", "job", "marital", "education", "default", "balance", "loan", "contact")
if (!all(expected_arguments %in% names(train_data))) {
stop("Column names in train_data do not match expected arguments in compile_text function.")
}
str(train_data)
# Removing NA values or any other preprocessing
train_data <- train_data %>% tidyr::drop_na()
# Convert to a list of sentences
compile_text <- function(age, housing, job, marital, education, default, balance, loan, contact) {
text <- paste0(
"Age: ", age, ", ",
"Housing load: ", housing, ", ",
"Job: ", job, ", ",
"Marital: ", marital, ", ",
"Education: ", education, ", ",
"Default: ", default, ", ",
"Balance: ", balance, ", ",
"Personal loan: ", loan, ", ",
"Contact: ", contact
)
return(text)
}
# Apply the function to create a list of sentences
sentences <- purrr::pmap_chr(train_data, compile_text)
# Create an iterator over tokens
tokens <- itoken(sentences,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = TRUE)
# Create vocabulary
vocab <- create_vocabulary(tokens)
# Create a vectorizer
vectorizer <- vocab_vectorizer(vocab)
# Create a term-co-occurrence matrix (TCM)
tcm <- create_tcm(tokens, vectorizer)
# Fit a GloVe model
glove_model <- GlobalVectors$new(rank = 50, x_max = 10)
word_vectors <- glove_model$fit_transform(tcm, n_iter = 20)
# Combine main and context vectors
word_vectors <- word_vectors + t(glove_model$components)
# Create an embedding for each sentence by averaging word vectors
sentence_embeddings <- function(sent, word_vectors) {
words <- unlist(word_tokenizer(tolower(sent)))
valid_words <- words[words %in% rownames(word_vectors)]
if (length(valid_words) == 0) return(rep(NA, ncol(word_vectors)))
word_vecs <- word_vectors[valid_words, , drop = FALSE]
colMeans(word_vecs, na.rm = TRUE)
}
# Apply the embedding function to all sentences
embeddings <- t(sapply(sentences, sentence_embeddings, word_vectors = word_vectors))
# Remove sentences that couldn't be embedded
embeddings <- embeddings[complete.cases(embeddings), ]
# Convert embeddings to data frame
embeddings_df <- as.data.frame(embeddings)
# Inspect the resulting embeddings
str(embeddings_df)
# Save embeddings to CSV for future use
write.csv(embeddings_df, "train_embeddings.csv", row.names = FALSE)
# Load data
embedding_data <- read.csv("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/LMM+kMean/train_embeddings.csv")
# Calculate z-scores for each column
z_scores <- apply(embedding_data, 2, function(x) (x - mean(x)) / sd(x))
# Find indices of outliers
outlier_indices <- apply(z_scores, 2, function(x) which(abs(x) > threshold))
# Flatten the list of outlier indices
all_outlier_indices <- unlist(outlier_indices)
# Remove duplicate indices
all_outlier_indices <- unique(all_outlier_indices)
# Remove outliers from embedding data
embedding_data_no_outliers <- embedding_data[-all_outlier_indices, ]
# Inspect the resulting embeddings without outliers
str(embedding_data_no_outliers)
# Optionally, save the embedding data without outliers to CSV
write.csv(embedding_data_no_outliers, "train_embeddings_no_outliers.csv", row.names = FALSE)
rm(list=ls())
library(readxl)
library(dplyr)
library(tidyr)
library(text2vec)
library(purrr)
# Load data
train_data <- read_csv("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/LMM+kMean/train_embeddings_no_outliers.csv)
library(readxl)
library(dplyr)
library(tidyr)
library(text2vec)
library(purrr)
# Load data
train_data <- read_csv("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/LMM+kMean/train_embeddings_no_outliers.csv")
# Load data
train_data <- read_csv("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/LMM+kMean/train_embeddings_no_outliers.csv")
rm(list=ls())
library(readxl)
library(dplyr)
library(tidyr)
library(text2vec)
library(purrr)
library(rio)  # Load the rio package
# Load data using rio's import function
train_data <- import("C:/Users/91884/Desktop/BAIS/Independent Study - Zantedeschi/exercise1/LMM+kMean/train_embeddings_no_outliers.csv")
# Perform elbow test
elbow_data <- data.frame(K = integer(), WCSS = numeric())
for (k in 1:10) {
kmeans_model <- kmeans(train_data, centers = k, nstart = 25)
elbow_data <- rbind(elbow_data, data.frame(K = k, WCSS = kmeans_model$tot.withinss))
}
# Plot the elbow curve
library(ggplot2)
ggplot(elbow_data, aes(x = K, y = WCSS)) +
geom_line() +
geom_point() +
labs(x = "Number of clusters (K)", y = "Within-Cluster Sum of Squares (WCSS)",
title = "Elbow Method for Optimal K in K-Means Clustering")
# Perform k-means clustering with k = 5
kmeans_model <- kmeans(train_data, centers = 5, nstart = 25)
# Add cluster labels to the original data
train_data_with_clusters <- cbind(train_data, Cluster = kmeans_model$cluster)
# View the cluster centers
cluster_centers <- kmeans_model$centers
print(cluster_centers)
# View the cluster sizes
cluster_sizes <- table(kmeans_model$cluster)
print(cluster_sizes)
# Calculate cluster means
cluster_means <- aggregate(. ~ Cluster, train_data_with_clusters, mean)
print(cluster_means)
library(ggplot2)
ggplot(train_data_with_clusters, aes(x = V1, y = V2, color = factor(Cluster))) +
geom_point() +
geom_point(data = as.data.frame(cluster_centers), aes(x = V1, y = V2), color = "black", size = 3, shape = 17) +
labs(x = "Embedding Dimension 1", y = "Embedding Dimension 2", title = "K-Means Clustering with K = 5")
Load necessary libraries
library(cluster)  # For silhouette_score
library(fpc)      # For dbs
library(fpc)      # For calinski
# Compute Davies-Bouldin score
davies_bouldin <- dbs(train_data, kmeans_model$cluster)
print(paste("Davies-Bouldin Score:", davies_bouldin))
# Compute Calinski-Harabasz score
calinski_harabasz <- calinski(train_data, kmeans_model$cluster)
print(paste("Calinski-Harabasz Score:", calinski_harabasz))
# Compute Silhouette score
silhouette <- silhouette_score(train_data, kmeans_model$cluster)
print(paste("Silhouette Score:", silhouette))
library(cluster)  # For silhouette_score
library(fpc)      # For calinski
# Compute Davies-Bouldin score
davies_bouldin <- Davies_Bouldin(train_data, kmeans_model$cluster)
print(paste("Davies-Bouldin Score:", davies_bouldin))
# Compute Calinski-Harabasz score
calinski_harabasz <- calinski.test(train_data, kmeans_model$cluster)$criterion
print(paste("Calinski-Harabasz Score:", calinski_harabasz))
# Compute Silhouette score
silhouette <- silhouette_score(train_data, kmeans_model$cluster)
print(paste("Silhouette Score:", silhouette))
# Load necessary libraries
library(cluster)  # For silhouette
library(fpc)      # For cluster.stats
# Compute Davies-Bouldin index and Calinski-Harabasz index
cluster_stats <- cluster.stats(train_data, kmeans_model$cluster)
davies_bouldin <- cluster_stats$clus.dindex
calinski_harabasz <- cluster_stats$ch
# Compute Silhouette score
silhouette <- silhouette(train_data, kmeans_model$cluster)$avg.width
# Print the scores
print(paste("Davies-Bouldin Score:", davies_bouldin))
print(paste("Calinski-Harabasz Score:", calinski_harabasz))
print(paste("Silhouette Score:", silhouette))
# Load necessary libraries
library(cluster)  # For silhouette
# Custom function to calculate Davies-Bouldin index
davies_bouldin_index <- function(data, clusters) {
n_clusters <- length(unique(clusters))
cluster_means <- tapply(data, clusters, colMeans)
within_cluster_variances <- tapply(data, clusters, function(cluster) {
apply((cluster - cluster_means[as.character(clusters), ])^2, 2, sum)
})
cluster_distances <- outer(seq_len(n_clusters), seq_len(n_clusters), function(i, j) {
if (i == j) return(0)
(sum((cluster_means[[i]] - cluster_means[[j]])^2) / (sqrt(within_cluster_variances[[i]]) + sqrt(within_cluster_variances[[j]])))
})
db_indices <- apply(cluster_distances, 1, max)
return(mean(db_indices))
}
# Custom function to calculate Calinski-Harabasz index
calinski_harabasz_index <- function(data, clusters) {
n_clusters <- length(unique(clusters))
cluster_means <- tapply(data, clusters, colMeans)
overall_mean <- colMeans(data)
between_cluster_variances <- sum(sapply(cluster_means, function(cluster_mean) {
sum((cluster_mean - overall_mean)^2)
}))
within_cluster_variances <- sum(sapply(seq_len(n_clusters), function(i) {
sum((data[clusters == i, ] - cluster_means[[i]])^2)
}))
return((between_cluster_variances / (n_clusters - 1)) / (within_cluster_variances / (length(data) - n_clusters)))
}
# Compute Davies-Bouldin index
davies_bouldin <- davies_bouldin_index(train_data, kmeans_model$cluster)
# Compute Calinski-Harabasz index
calinski_harabasz <- calinski_harabasz_index(train_data, kmeans_model$cluster)
# Compute Silhouette score
silhouette <- silhouette(train_data, kmeans_model$cluster)$avg.width
# Print the scores
print(paste("Davies-Bouldin Score:", davies_bouldin))
print(paste("Calinski-Harabasz Score:", calinski_harabasz))
print(paste("Silhouette Score:", silhouette))
# Custom function to calculate Davies-Bouldin index
davies_bouldin_index <- function(data, clusters) {
n_clusters <- length(unique(clusters))
cluster_means <- tapply(data, clusters, colMeans)
within_cluster_variances <- tapply(data, clusters, function(cluster) {
apply((cluster - cluster_means[as.character(clusters), ])^2, 2, sum)
})
cluster_distances <- outer(seq_len(n_clusters), seq_len(n_clusters), function(i, j) {
if (i == j) return(0)
(sum((cluster_means[[i]] - cluster_means[[j]])^2) / (sqrt(within_cluster_variances[[i]]) + sqrt(within_cluster_variances[[j]])))
})
db_indices <- apply(cluster_distances, 1, max)
return(mean(db_indices))
}
# Custom function to calculate Calinski-Harabasz index
calinski_harabasz_index <- function(data, clusters) {
n_clusters <- length(unique(clusters))
cluster_means <- tapply(data, clusters, colMeans)
overall_mean <- colMeans(data)
between_cluster_variances <- sum(sapply(cluster_means, function(cluster_mean) {
sum((cluster_mean - overall_mean)^2)
}))
within_cluster_variances <- sum(sapply(seq_len(n_clusters), function(i) {
sum((data[clusters == i, ] - cluster_means[[i]])^2)
}))
return((between_cluster_variances / (n_clusters - 1)) / (within_cluster_variances / (length(data) - n_clusters)))
}
# Compute Davies-Bouldin index
davies_bouldin <- davies_bouldin_index(train_data, kmeans_model$cluster)
# Compute Calinski-Harabasz index
calinski_harabasz <- calinski_harabasz_index(train_data, kmeans_model$cluster)
# Compute Silhouette score
silhouette <- silhouette(train_data, kmeans_model$cluster)$avg.width
# Print the scores
print(paste("Davies-Bouldin Score:", davies_bouldin))
print(paste("Calinski-Harabasz Score:", calinski_harabasz))
print(paste("Silhouette Score:", silhouette))
davies_bouldin_index <- function(data, clusters) {
n_clusters <- length(unique(clusters))
cluster_means <- tapply(data, clusters, colMeans)
within_cluster_variances <- tapply(data, clusters, function(cluster) {
apply((cluster - cluster_means[as.character(clusters), ])^2, 2, sum)
})
cluster_distances <- outer(seq_len(n_clusters), seq_len(n_clusters), function(i, j) {
if (i == j) return(0)
(sum((cluster_means[[i]] - cluster_means[[j]])^2) / (sqrt(within_cluster_variances[[i]]) + sqrt(within_cluster_variances[[j]])))
})
db_indices <- apply(cluster_distances, 1, max)
return(mean(db_indices))
}
# Custom function to calculate Calinski-Harabasz index
calinski_harabasz_index <- function(data, clusters) {
n_clusters <- length(unique(clusters))
cluster_means <- tapply(data, clusters, colMeans)
overall_mean <- colMeans(data)
between_cluster_variances <- sum(sapply(cluster_means, function(cluster_mean) {
sum((cluster_mean - overall_mean)^2)
}))
within_cluster_variances <- sum(sapply(seq_len(n_clusters), function(i) {
sum((data[clusters == i, ] - cluster_means[[i]])^2)
}))
return((between_cluster_variances / (n_clusters - 1)) / (within_cluster_variances / (length(data) - n_clusters)))
}
# Compute Davies-Bouldin index
davies_bouldin <- davies_bouldin_index(train_data, as.integer(kmeans_model$cluster))
# Compute Calinski-Harabasz index
calinski_harabasz <- calinski_harabasz_index(train_data, as.integer(kmeans_model$cluster))
# Compute Silhouette score
silhouette <- silhouette(train_data, as.integer(kmeans_model$cluster))$avg.width
# Print the scores
print(paste("Davies-Bouldin Score:", davies_bouldin))
print(paste("Calinski-Harabasz Score:", calinski_harabasz))
print(paste("Silhouette Score:", silhouette))
install.packages("fpc")
library(fpc)      # For cluster.stats
# Custom function to calculate Davies-Bouldin index
davies_bouldin_index <- function(data, clusters) {
n_clusters <- length(unique(clusters))
cluster_means <- tapply(data, clusters, colMeans)
within_cluster_variances <- tapply(data, clusters, function(cluster) {
apply((cluster - cluster_means[as.character(clusters), ])^2, 2, sum)
})
cluster_distances <- outer(seq_len(n_clusters), seq_len(n_clusters), function(i, j) {
if (i == j) return(0)
(sum((cluster_means[[i]] - cluster_means[[j]])^2) / (sqrt(within_cluster_variances[[i]]) + sqrt(within_cluster_variances[[j]])))
})
db_indices <- apply(cluster_distances, 1, max)
return(mean(db_indices))
}
# Custom function to calculate Calinski-Harabasz index
calinski_harabasz_index <- function(data, clusters) {
n_clusters <- length(unique(clusters))
cluster_means <- tapply(data, clusters, colMeans)
overall_mean <- colMeans(data)
between_cluster_variances <- sum(sapply(cluster_means, function(cluster_mean) {
sum((cluster_mean - overall_mean)^2)
}))
within_cluster_variances <- sum(sapply(seq_len(n_clusters), function(i) {
sum((data[clusters == i, ] - cluster_means[[i]])^2)
}))
return((between_cluster_variances / (n_clusters - 1)) / (within_cluster_variances / (length(data) - n_clusters)))
}
# Ensure train_data contains numeric values
train_data <- as.matrix(train_data)
# Compute Davies-Bouldin index
davies_bouldin <- davies_bouldin_index(train_data, as.integer(kmeans_model$cluster))
# Compute Calinski-Harabasz index
calinski_harabasz <- cluster.stats(train_data, as.integer(kmeans_model$cluster))$ch
# Compute Silhouette score
silhouette <- silhouette(train_data, as.integer(kmeans_model$cluster))$avg.width
# Print the scores
print(paste("Davies-Bouldin Score:", davies_bouldin))
print(paste("Calinski-Harabasz Score:", calinski_harabasz))
print(paste("Silhouette Score:", silhouette))
